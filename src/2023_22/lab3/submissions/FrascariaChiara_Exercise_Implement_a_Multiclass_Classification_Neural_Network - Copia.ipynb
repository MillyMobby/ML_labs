{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5AYt3Cn35zA"
      },
      "source": [
        "**Challenge: Implement a Multiclass Classification Neural Network using PyTorch**\n",
        "\n",
        "Objective:\n",
        "Build a neural network using PyTorch to predict handwritten digits of MNIST.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. **Data Preparation**: Load the MNIST dataset using ```torchvision.datasets.MNIST```. Standardize/normalize the features. Split the dataset into training and testing sets using, for example, ```sklearn.model_selection.train_test_split()```. **Bonus scores**: *use PyTorch's built-* ```DataLoader``` *to split the dataset*.\n",
        "\n",
        "2. **Neural Network Architecture**: Define a simple feedforward neural network using PyTorch's ```nn.Module```. Design the input layer to match the number of features in the MNIST dataset and the output layer to have as many neurons as there are classes (10). You can experiment with the number of hidden layers and neurons to optimize the performance. **Bonus scores**: *Make your architecture flexibile to have as many hidden layers as the user wants, and use hyperparameter optimization to select the best number of hidden layeres.*\n",
        "\n",
        "3. **Loss Function and Optimizer**: Choose an appropriate loss function for multiclass classification. Select an optimizer, like SGD (Stochastic Gradient Descent) or Adam.\n",
        "\n",
        "4. **Training**: Write a training loop to iterate over the dataset.\n",
        "Forward pass the input through the network, calculate the loss, and perform backpropagation. Update the weights of the network using the chosen optimizer.\n",
        "\n",
        "5. **Testing**: Evaluate the trained model on the test set. Calculate the accuracy of the model.\n",
        "\n",
        "6. **Optimization**: Experiment with hyperparameters (learning rate, number of epochs, etc.) to optimize the model's performance. Consider adjusting the neural network architecture for better results. **Notice that you can't use the optimization algorithms from scikit-learn that we saw in lab1: e.g.,** ```GridSearchCV```.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5XLynrxJ33v6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60000\n",
            "10000\n",
            "Epoch 1 --> loss = 0.3032480383555947\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Chiara Frascaria\\OneDrive\\Desktop\\naja\\Lab1ML\\FrascariaChiara_Exercise_Implement_a_Multiclass_Classification_Neural_Network.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chiara%20Frascaria/OneDrive/Desktop/naja/Lab1ML/FrascariaChiara_Exercise_Implement_a_Multiclass_Classification_Neural_Network.ipynb#W2sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chiara%20Frascaria/OneDrive/Desktop/naja/Lab1ML/FrascariaChiara_Exercise_Implement_a_Multiclass_Classification_Neural_Network.ipynb#W2sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39m# Loop over the dataset in batches\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chiara%20Frascaria/OneDrive/Desktop/naja/Lab1ML/FrascariaChiara_Exercise_Implement_a_Multiclass_Classification_Neural_Network.ipynb#W2sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m      losses \u001b[39m=\u001b[39m []\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Chiara%20Frascaria/OneDrive/Desktop/naja/Lab1ML/FrascariaChiara_Exercise_Implement_a_Multiclass_Classification_Neural_Network.ipynb#W2sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m      \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m trainloader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chiara%20Frascaria/OneDrive/Desktop/naja/Lab1ML/FrascariaChiara_Exercise_Implement_a_Multiclass_Classification_Neural_Network.ipynb#W2sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m         \u001b[39m# forward, backward, and then weight update\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chiara%20Frascaria/OneDrive/Desktop/naja/Lab1ML/FrascariaChiara_Exercise_Implement_a_Multiclass_Classification_Neural_Network.ipynb#W2sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m         y_pred \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chiara%20Frascaria/OneDrive/Desktop/naja/Lab1ML/FrascariaChiara_Exercise_Implement_a_Multiclass_Classification_Neural_Network.ipynb#W2sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m         loss \u001b[39m=\u001b[39m loss_fn(y_pred, labels)\n",
            "File \u001b[1;32mc:\\Users\\Chiara Frascaria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\Chiara Frascaria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\Chiara Frascaria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\Chiara Frascaria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\Chiara Frascaria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[1;32mc:\\Users\\Chiara Frascaria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
            "File \u001b[1;32mc:\\Users\\Chiara Frascaria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
            "File \u001b[1;32mc:\\Users\\Chiara Frascaria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:172\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    170\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mview(pic\u001b[39m.\u001b[39msize[\u001b[39m1\u001b[39m], pic\u001b[39m.\u001b[39msize[\u001b[39m0\u001b[39m], F_pil\u001b[39m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    171\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mpermute((\u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[0;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mdefault_float_dtype)\u001b[39m.\u001b[39mdiv(\u001b[39m255\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# insert code here\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),  \n",
        "    torchvision.transforms.Normalize((0.5,), (0.5,))  # Normalizza i valori dei pixel nell'intervallo [-1, 1]\n",
        "])\n",
        "\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
        "\n",
        "print(len(trainset))\n",
        "print(len(testset))\n",
        "\n",
        "class MNISTConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1= nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, \n",
        "                              stride=1, padding=1) \n",
        "        self.act1=nn.ReLU()\n",
        "        \n",
        "        self.conv2= nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool2=nn.MaxPool2d(kernel_size=2)\n",
        "        self.act2= nn.ReLU()\n",
        "\n",
        "        #(32, h, h)\n",
        "        self.flat= nn.Flatten()\n",
        "        self.fc3= nn.Linear(588, 512)\n",
        "        self.act3= nn.ReLU()\n",
        "\n",
        "        self.fc4= nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input 1 x 28 x 28, output 3 x 28 x 28\n",
        "        x=self.conv1(x)\n",
        "        x=self.act1(x)\n",
        "            # input 3 x 28 x 28, output 3 x 28 x 28\n",
        "        x=self.conv2(x)\n",
        "        x=self.act2(x)\n",
        "        # input 32 x 32 x 32, output 32 x 16 x 16\n",
        "        x=self.pool2(x)\n",
        "        # 32 x 16 x 16, 8192\n",
        "        x=self.flat(x)\n",
        "        x=self.fc3(x)\n",
        "        x=self.act3(x)\n",
        "        x=self.fc4(x)\n",
        "        return x\n",
        "\n",
        "model = MNISTConvNet()\n",
        "model\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "n_epochs = 100\n",
        "batch_size = 10\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Loop over the dataset in batches\n",
        "     losses = []\n",
        "     for inputs, labels in trainloader:\n",
        "        # forward, backward, and then weight update\n",
        "        y_pred = model(inputs)\n",
        "        loss = loss_fn(y_pred, labels)\n",
        "        losses.append(loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "     print(f'Epoch {epoch + 1} --> loss = {np.mean(losses)}')\n",
        "\n",
        "acc = 0\n",
        "count = 0\n",
        "for inputs, labels in testloader:\n",
        "    y_pred = model(inputs)\n",
        "    acc += (torch.argmax(y_pred, 1) == labels).float().sum()\n",
        "    count += len(labels)\n",
        "    acc /= count\n",
        "print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, acc*100))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
